{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost Play Prediction Model Training\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "import cupy as cp\n",
    "import xgboost as xgb\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import nflpp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use cuda for accelerated training if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Data\n",
    "\n",
    "### Load the play-by-play data\n",
    "The data for this project is provided by the [nfl_data_py](https://github.com/nflverse/nfl_data_py) python library. The dataset covers the NFL seasons from 1999 to 2023, so we'll load all years in that range. A function from this project's `nflpp` module is used to ensure the data is cached and loaded from cache at all times for better peformance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "pbp_data = nflpp.load_pbp_data(range(1999, 2024))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter excess data\n",
    "The vast majority of plays called in an NFL game are either runs or passes. Other types of plays are either forced by game rules (kickoffs) or almost always shown pre-snap by formation (field goals and punts). For these reasons, the play prediction model will focus only on play calls involving a run or pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = pbp_data[pbp_data[\"play_type\"].isin([\"run\", \"pass\"])]\n",
    "\n",
    "print(f\"Run/pass filtered shape: {processed_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The play-by-play data has a large amount of columns that can't be used for this project. The goal is to have the model be used to predict the next play call within the 40 seconds the team has to snap the ball. Many columns, such as `yards_gained`, contain information about the outcome of the play. Others contain data that would not be able to be processed pre-snap, like `wp` (win percentage). Because of this, the data will be filtered down to the most important columns that can be easily gathered pre-snap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\n",
    "    # Target\n",
    "    \"play_type\",\n",
    "    # Features\n",
    "    \"drive\",\n",
    "    \"qtr\",\n",
    "    \"quarter_seconds_remaining\",\n",
    "    \"down\",\n",
    "    \"ydstogo\",\n",
    "    \"yardline_100\",\n",
    "    \"shotgun\",\n",
    "    \"no_huddle\",\n",
    "    \"score_differential\",\n",
    "    \"spread_line\",\n",
    "    \"season\",\n",
    "    \"posteam\",\n",
    "    # Used to create poscoach\n",
    "    \"home_coach\",\n",
    "    \"away_coach\",\n",
    "    \"home_team\",\n",
    "    \"away_team\",\n",
    "]\n",
    "processed_data = processed_data[columns]\n",
    "print(f\"Run/pass filtered shape: {processed_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create new column\n",
    "\n",
    "While the data contains a variable for the team currently in possession of the ball, no such variable exists for the coach. This will create a new column for the possessing team's coach, and then remove the columns used its creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data.loc[:, \"poscoach\"] = processed_data.apply(\n",
    "    lambda row: row[\"home_coach\"]\n",
    "    if row[\"posteam\"] == row[\"home_team\"]\n",
    "    else row[\"away_coach\"],\n",
    "    axis=1,\n",
    ")\n",
    "processed_data = processed_data.drop(\n",
    "    columns=[\"home_coach\", \"away_coach\", \"home_team\", \"away_team\"]\n",
    ")\n",
    "print(f\"Post-poscoach shape: {processed_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode descriptive features\n",
    "\n",
    "XGBoost needs data to be numerical, so the variables that are present in the dataset need to be encoded to integer values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in (\"posteam\", \"poscoach\", \"play_type\"):\n",
    "    processed_data[column] = LabelEncoder().fit_transform(processed_data[column])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partition data for training\n",
    "\n",
    "In order to train the model, the data must be split for evaluation after training. Since the use-case of the model will be to predict future results based on previous data, the final season will be used as the evaluation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = processed_data[processed_data[\"season\"] < 2022]\n",
    "test_data = processed_data[processed_data[\"season\"] == 2022]\n",
    "validation_data = processed_data[processed_data[\"season\"] == 2023]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be used for training functions, the training and testing datasets must be further split into the features and the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = train_data.drop(\"play_type\", axis=1)\n",
    "train_target = train_data[\"play_type\"]\n",
    "test_features = train_data.drop(\"play_type\", axis=1)\n",
    "test_target = train_data[\"play_type\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if device == \"cuda\":\n",
    "    train_features = cp.asarray(train_features)\n",
    "    train_target = cp.asarray(train_target)\n",
    "    test_features = cp.asarray(test_features)\n",
    "    test_target = cp.asarray(test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(train_features, label=train_target)\n",
    "dtest = xgb.DMatrix(test_features, label=test_target)\n",
    "\n",
    "eval_set = [(dtrain, \"train\"), (dtest, \"test\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_parameters = {\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"device\": device,\n",
    "    \"eval_metric\": [\"auc\", \"error\", \"logloss\"],\n",
    "}\n",
    "model = xgb.train(model_parameters, dtrain, num_boost_round=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = (model.predict(dtest) > 0.5).astype(int)\n",
    "\n",
    "accuracy = accuracy_score(test_target, test_preds)\n",
    "f1 = f1_score(test_target, test_preds)\n",
    "\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(f\"F1 Score: {f1:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the model\n",
    "\n",
    "After training, the XGBoost model is saved in JSON file found within the /models directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_directory = \"../models\"\n",
    "\n",
    "version = \"1.1.1\"\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "filename = f\"{models_directory}/xgboost_v{version}_{timestamp}_{accuracy:.4f}.json\"\n",
    "\n",
    "model.save_model(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Analysis\n",
    "\n",
    "The rest of the notebook is devoted to evaluating the performance of the trained model in greater detail. These model analyses are used to compare each iteration of the model in order to focus future developments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nflpp.plot_confusion_matrix(test_target, test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities = clf.predict_proba(test_features)\n",
    "nflpp.plot_roc_curve(test_target, probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = train_data.drop(\"play_type\", axis=1).columns.tolist()\n",
    "clf.get_booster().feature_names = features\n",
    "plot_importance(clf)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = clf.evals_result()\n",
    "nflpp.plot_learning_curves(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
